from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
import asyncio
import json
import logging
import time

from app.routers.auth_db import get_current_user
from app.core.database import get_redis_client
from app.core.config import SETTINGS

from app.services.queue_service import get_queue_service, QueueService

router = APIRouter()
logger = logging.getLogger("webapi.sse")


async def task_progress_generator(task_id: str, user_id: str):
    """Generate SSE events for task progress updates"""
    r = get_redis_client()
    pubsub = None
    channel = f"task_progress:{task_id}"

    try:
        # Load dynamic SSE settings
        try:
            from app.services.config_provider import provider as config_provider
            eff = await config_provider.get_effective_system_settings()
            poll_timeout = float(eff.get("sse_poll_timeout_seconds", 1.0))
            heartbeat_every = int(eff.get("sse_heartbeat_interval_seconds", 10))
            max_idle_seconds = int(eff.get("sse_task_max_idle_seconds", 300))
        except Exception:
            poll_timeout = float(getattr(SETTINGS, "SSE_POLL_TIMEOUT_SECONDS", 1.0))
            heartbeat_every = int(getattr(SETTINGS, "SSE_HEARTBEAT_INTERVAL_SECONDS", 10))
            max_idle_seconds = int(getattr(SETTINGS, "SSE_TASK_MAX_IDLE_SECONDS", 300))

        #Fix: Create PubSub Connection
        pubsub = r.pubsub()
        logger.info(f"[SSE-Task] Create PubSub Connection:{task_id}, user={user_id}")

        #Fixing: Subscription Channels (possible failure, need to ensure that pubsub is cleaned)
        try:
            await pubsub.subscribe(channel)
            logger.info(f"[SSE-Task] Subscription channel successfully:{channel}")
            # Send initial connection confirmation
            yield f"event: connected\ndata: {{\"task_id\": \"{task_id}\", \"message\": \"Â∑≤ËøûÊé•ËøõÂ∫¶ÊµÅ\"}}\n\n"
        except Exception as subscribe_error:
            #üî• Cleanup pubsub connection as soon as the subscription fails
            logger.error(f"[SSE-Task]{subscribe_error}")
            try:
                await pubsub.close()
                logger.info(f"Could not close temporary folder: %s")
            except Exception as close_error:
                logger.error(f"Could not close temporary folder: %s{close_error}")
            #Releasing anomalies for except processing
            raise

        # Listen for progress updates
        idle_elapsed = 0.0
        last_hb = time.monotonic()

        while idle_elapsed < max_idle_seconds:
            try:
                message = await asyncio.wait_for(pubsub.get_message(ignore_subscribe_messages=True), timeout=poll_timeout)
                if message and message['type'] == 'message':
                    # Reset idle timer on valid message
                    idle_elapsed = 0.0
                    try:
                        progress_data = json.loads(message['data'])
                        yield f"event: progress\ndata: {json.dumps(progress_data, ensure_ascii=False)}\n\n"
                    except json.JSONDecodeError:
                        logger.warning(f"Invalid JSON in progress message: {message['data']}")
                else:
                    # No update: accumulate idle time and send heartbeat if due
                    idle_elapsed += poll_timeout
                    now = time.monotonic()
                    if now - last_hb >= heartbeat_every:
                        yield f"event: heartbeat\ndata: {{\"timestamp\": \"{asyncio.get_event_loop().time()}\"}}\n\n"
                        last_hb = now

            except asyncio.TimeoutError:
                idle_elapsed += poll_timeout
                continue

    except Exception as e:
        logger.exception(f"SSE error for task {task_id}: {e}")
        yield f"event: error\ndata: {{\"error\": \"ËøûÊé•ÂºÇÂ∏∏: {str(e)}\"}}\n\n"
    finally:
        #Rehabilitation: ensure that connections are released in all circumstances
        if pubsub:
            logger.info(f"[SSE-Task] Cleanup PubSub Connection:{task_id}")

            #Close in stages to ensure that the connection is closed even if unsubscribe fails
            try:
                await pubsub.unsubscribe(channel)
                logger.debug(f"[SSE-Task]{channel}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [SSE-Task] Unsubscription failed (continue to close the connection):{e}")

            try:
                await pubsub.close()
                logger.info(f"[SSE-Task] PubSub connection closed:{task_id}")
            except Exception as e:
                logger.error(f"Could not close temporary folder: %s{e}", exc_info=True)
                #Even if closing failed, try to reset the connection
                try:
                    await pubsub.reset()
                    logger.info(f"[SSE-Task] PubSub connection has been reset:{task_id}")
                except Exception as reset_error:
                    logger.error(f"[SSE-Task] Reset the PubSub connection also failed:{reset_error}")


async def batch_progress_generator(batch_id: str, user_id: str):
    """Generate SSE events for batch progress updates"""
    svc = get_queue_service()

    try:
        # Load dynamic SSE settings for batch stream
        try:
            from app.services.config_provider import provider as config_provider
            eff = await config_provider.get_effective_system_settings()
            batch_poll_interval = float(eff.get("sse_batch_poll_interval_seconds", 2))
            batch_max_idle_seconds = int(eff.get("sse_batch_max_idle_seconds", 600))
        except Exception:
            batch_poll_interval = float(getattr(SETTINGS, "SSE_BATCH_POLL_INTERVAL_SECONDS", 2.0))
            batch_max_idle_seconds = int(getattr(SETTINGS, "SSE_BATCH_MAX_IDLE_SECONDS", 600))

        # Send initial connection confirmation
        yield f"event: connected\ndata: {{\"batch_id\": \"{batch_id}\", \"message\": \"Â∑≤ËøûÊé•ÊâπÊ¨°ËøõÂ∫¶ÊµÅ\"}}\n\n"

        idle_elapsed = 0.0

        while idle_elapsed < batch_max_idle_seconds:
            try:
                # Get current batch status
                batch_data = await svc.get_batch(batch_id)
                if not batch_data:
                    yield f"event: error\ndata: {{\"error\": \"ÊâπÊ¨°‰∏çÂ≠òÂú®\"}}\n\n"
                    break

                # Check if batch belongs to user
                if batch_data.get("user") != user_id:
                    yield f"event: error\ndata: {{\"error\": \"Êó†ÊùÉÈôêËÆøÈóÆÊ≠§ÊâπÊ¨°\"}}\n\n"
                    break

                # Calculate batch progress based on task statuses
                task_ids = batch_data.get("tasks", [])
                if not task_ids:
                    yield f"event: progress\ndata: {{\"batch_id\": \"{batch_id}\", \"message\": \"ÊâπÊ¨°Êó†‰ªªÂä°\", \"progress\": 0}}\n\n"
                    await asyncio.sleep(batch_poll_interval)
                    idle_elapsed += batch_poll_interval
                    continue

                completed_count = 0
                failed_count = 0
                processing_count = 0

                for task_id in task_ids:
                    task_data = await svc.get_task(task_id)
                    if task_data:
                        status = task_data.get("status", "queued")
                        if status == "completed":
                            completed_count += 1
                        elif status == "failed":
                            failed_count += 1
                        elif status == "processing":
                            processing_count += 1

                total_tasks = len(task_ids)
                finished_tasks = completed_count + failed_count
                progress = round((finished_tasks / total_tasks) * 100, 1) if total_tasks > 0 else 0

                # Determine batch status
                if finished_tasks == total_tasks:
                    if failed_count == 0:
                        batch_status = "completed"
                        message = f"ÊâπÊ¨°ÂÆåÊàê: {completed_count}/{total_tasks} ÊàêÂäü"
                    elif completed_count == 0:
                        batch_status = "failed"
                        message = f"ÊâπÊ¨°Â§±Ë¥•: {failed_count}/{total_tasks} Â§±Ë¥•"
                    else:
                        batch_status = "partial"
                        message = f"ÊâπÊ¨°ÈÉ®ÂàÜÊàêÂäü: {completed_count} ÊàêÂäü, {failed_count} Â§±Ë¥•"
                elif processing_count > 0 or finished_tasks < total_tasks:
                    batch_status = "processing"
                    message = f"ÊâπÊ¨°Â§ÑÁêÜ‰∏≠: {finished_tasks}/{total_tasks} Â∑≤ÂÆåÊàê, {processing_count} Â§ÑÁêÜ‰∏≠"
                else:
                    batch_status = "queued"
                    message = f"ÊâπÊ¨°ÊéíÈòü‰∏≠: {total_tasks} ‰ªªÂä°ÂæÖÂ§ÑÁêÜ"

                progress_data = {
                    "batch_id": batch_id,
                    "status": batch_status,
                    "message": message,
                    "progress": progress,
                    "total_tasks": total_tasks,
                    "completed": completed_count,
                    "failed": failed_count,
                    "processing": processing_count,
                    "timestamp": asyncio.get_event_loop().time()
                }

                yield f"event: progress\ndata: {json.dumps(progress_data, ensure_ascii=False)}\n\n"

                # Break if batch is finished
                if batch_status in ["completed", "failed", "partial"]:
                    yield f"event: finished\ndata: {{\"batch_id\": \"{batch_id}\", \"final_status\": \"{batch_status}\"}}\n\n"
                    break

                # Wait before next update
                await asyncio.sleep(batch_poll_interval)
                idle_elapsed += batch_poll_interval

            except Exception as e:
                logger.exception(f"Batch progress error: {e}")
                yield f"event: error\ndata: {{\"error\": \"Ëé∑ÂèñÊâπÊ¨°Áä∂ÊÄÅÂ§±Ë¥•: {str(e)}\"}}\n\n"
                break

    except Exception as e:
        logger.exception(f"SSE batch error for {batch_id}: {e}")
        yield f"event: error\ndata: {{\"error\": \"ËøûÊé•ÂºÇÂ∏∏: {str(e)}\"}}\n\n"


@router.get("/tasks/{task_id}")
async def stream_task_progress(task_id: str, user: dict = Depends(get_current_user), svc: QueueService = Depends(get_queue_service)):
    """Stream real-time progress updates for a specific task"""
    # Verify task exists and belongs to user
    task_data = await svc.get_task(task_id)
    if not task_data or task_data.get("user") != user["id"]:
        raise HTTPException(status_code=404, detail="Task not found")

    return StreamingResponse(
        task_progress_generator(task_id, user["id"]),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )


@router.get("/batches/{batch_id}")
async def stream_batch_progress(batch_id: str, user: dict = Depends(get_current_user), svc: QueueService = Depends(get_queue_service)):
    """Stream real-time progress updates for a batch"""
    # Verify batch exists and belongs to user
    batch_data = await svc.get_batch(batch_id)
    if not batch_data or batch_data.get("user") != user["id"]:
        raise HTTPException(status_code=404, detail="Batch not found")

    return StreamingResponse(
        batch_progress_generator(batch_id, user["id"]),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )